version: '3.8'

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         NEXUS LLM Stack - Docker Compose                    ║
# ╚══════════════════════════════════════════════════════════════════════════╝

services:
  # Ollama - Local LLM inference
  ollama:
    image: ollama/ollama:latest
    container_name: nexus-ollama
    restart: unless-stopped
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./models:/models
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    gpus: all
    networks:
      - llm-network

  # vLLM - High-performance inference server
  vllm:
    image: vllm/vllm-openai:latest
    container_name: nexus-vllm
    restart: unless-stopped
    ports:
      - "127.0.0.1:8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
    environment:
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.2
      --dtype auto
      --api-key token-abc123
    gpus: all
    shm_size: '16gb'
    networks:
      - llm-network

  # LocalAI - OpenAI compatible API
  localai:
    image: quay.io/go-skynet/local-ai:latest-aio-gpu-nvidia-cuda-12
    container_name: nexus-localai
    restart: unless-stopped
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - ./models:/models
      - ./images:/tmp/generated/images
    environment:
      - THREADS=8
      - CONTEXT_SIZE=4096
      - DEBUG=false
      - GALLERIES=[{"name":"model-gallery","url":"github:go-skynet/model-gallery/index.yaml"}]
    gpus: all
    networks:
      - llm-network

  # Text Generation WebUI
  text-generation-webui:
    image: atinoda/text-generation-webui:default-nvidia
    container_name: nexus-webui
    restart: unless-stopped
    ports:
      - "127.0.0.1:7860:7860"
      - "127.0.0.1:5000:5000"  # API
      - "127.0.0.1:5005:5005"  # Streaming API
    volumes:
      - ./models:/app/models
      - ./characters:/app/characters
      - ./extensions:/app/extensions
      - ./presets:/app/presets
    environment:
      - CLI_ARGS=--listen --api --extensions openai
    gpus: all
    networks:
      - llm-network

  # Open WebUI (formerly Ollama WebUI)
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: nexus-open-webui
    restart: unless-stopped
    ports:
      - "127.0.0.1:3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434/api
      - WEBUI_AUTH=false
    depends_on:
      - ollama
    networks:
      - llm-network

  # Jupyter for ML experiments
  jupyter:
    image: jupyter/tensorflow-notebook:latest
    container_name: nexus-jupyter
    restart: unless-stopped
    ports:
      - "127.0.0.1:8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./models:/home/jovyan/models
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=nexus2024
    gpus: all
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge
    name: nexus-llm

volumes:
  ollama_data:
  open-webui:

#!/bin/bash
set -euo pipefail
IFS=$'\n\t'

# Setup Slurm (minimal, no accounting) + Tailscale SSH + tagging on Arch
# Requires: sudo privileges

usage() {
  cat <<'EOF'
Usage: sudo ./scripts/setup-slurm-tailscale.sh [options]

Options:
  --role [controller|node]   Node role (default: controller)
  --cluster-name NAME        Slurm ClusterName (default: fsnet)
  --controller HOST          Slurm ControlMachine hostname (default: current host if controller)
  --tags TAGS                Tailscale tags, comma-separated (default: tag:fsws)
  --hostname NAME            Tailscale advertised hostname (default: current hostname)
  --authkey KEY              Tailscale auth key (from admin console)
  --munge-key PATH           Path to an existing munge.key (for nodes)
  --with-accounting          Prepare accounting placeholders (you still need MariaDB)
  -h, --help                 Show this help

Notes:
  - This script installs Slurm from AUR (via yay), plus munge, tailscale, chrony.
  - Default config runs without accounting (no MariaDB). You can add later.
EOF
}

ROLE="controller"
CLUSTER_NAME="fsnet"
CONTROLLER_HOST=""
TAGS="tag:fsws"
TS_HOSTNAME="$(hostname)"
AUTHKEY=""
MUNGE_KEY_SRC=""
WITH_ACCOUNTING="false"

while [ $# -gt 0 ]; do
  case "$1" in
    --role) ROLE="$2"; shift 2;;
    --cluster-name) CLUSTER_NAME="$2"; shift 2;;
    --controller) CONTROLLER_HOST="$2"; shift 2;;
    --tags) TAGS="$2"; shift 2;;
    --hostname) TS_HOSTNAME="$2"; shift 2;;
    --authkey) AUTHKEY="$2"; shift 2;;
    --munge-key) MUNGE_KEY_SRC="$2"; shift 2;;
    --with-accounting) WITH_ACCOUNTING="true"; shift 1;;
    -h|--help) usage; exit 0;;
    *) echo "Unknown option: $1"; usage; exit 1;;
  esac
done

if [ "$(id -u)" -ne 0 ]; then
  echo "Please run with sudo: sudo ./scripts/setup-slurm-tailscale.sh ..." >&2
  exit 1
fi

echo "==> Installing base packages (tailscale, munge, chrony, jq)"
pacman -S --needed --noconfirm tailscale munge chrony jq || true

if ! command -v yay >/dev/null 2>&1; then
  echo "==> Installing yay (AUR helper)"
  sudo -u "${SUDO_USER:-$USER}" bash -lc 'git clone https://aur.archlinux.org/yay.git /tmp/yay && cd /tmp/yay && makepkg -si --noconfirm && rm -rf /tmp/yay'
fi

echo "==> Installing Slurm from AUR"
sudo -u "${SUDO_USER:-$USER}" bash -lc 'yay -S --needed --noconfirm slurm'

echo "==> Enabling services: tailscaled, chronyd, munge"
systemctl enable --now tailscaled
systemctl enable --now chronyd
systemctl enable --now munge || true

echo "==> Configuring Tailscale"
if [ -n "$AUTHKEY" ]; then
  tailscale up --authkey "$AUTHKEY" --ssh --advertise-tags="$TAGS" --hostname "$TS_HOSTNAME" || true
else
  echo "No --authkey provided. You can run later:"
  echo "  sudo tailscale up --ssh --advertise-tags=$TAGS --hostname $TS_HOSTNAME"
fi

TS_IP4=$(tailscale ip -4 2>/dev/null | head -n1 || true)
TS_NODE_NAME=$(tailscale status --peers=false 2>/dev/null | awk 'NR==1{print $1}' || hostname)
echo "Tailscale IPv4: ${TS_IP4:-N/A}  Node: ${TS_NODE_NAME}"

HOSTNAME_NOW=$(hostname)
CORES=$(nproc)
MEM_KB=$(grep MemTotal /proc/meminfo | awk '{print $2}')
# Reserve 1024 MiB for OS
REAL_MEM_MB=$(( (MEM_KB/1024) - 1024 ))
if [ $REAL_MEM_MB -lt 1024 ]; then REAL_MEM_MB=1024; fi

mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
chown -R slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm || true

if [ ! -f /etc/munge/munge.key ]; then
  if [ -n "$MUNGE_KEY_SRC" ] && [ -f "$MUNGE_KEY_SRC" ]; then
    echo "==> Installing provided munge key"
    install -o munge -g munge -m 0400 "$MUNGE_KEY_SRC" /etc/munge/munge.key
  else
    echo "==> Generating new munge key"
    dd if=/dev/urandom bs=1 count=1024 of=/etc/munge/munge.key
    chown munge:munge /etc/munge/munge.key
    chmod 0400 /etc/munge/munge.key
  fi
  systemctl restart munge
fi

if [ "$ROLE" = "controller" ]; then
  CONTROL_MACHINE="$HOSTNAME_NOW"
else
  if [ -z "$CONTROLLER_HOST" ]; then
    echo "--controller is required when role=node" >&2
    exit 1
  fi
  CONTROL_MACHINE="$CONTROLLER_HOST"
fi

echo "==> Writing /etc/slurm/slurm.conf (role=$ROLE)"
cat > /etc/slurm/slurm.conf <<EOF
# Generated by setup-slurm-tailscale.sh
ClusterName=$CLUSTER_NAME
ControlMachine=$CONTROL_MACHINE
SlurmUser=slurm
SlurmdUser=root
AuthType=auth/munge
StateSaveLocation=/var/spool/slurmctld
SlurmdSpoolDir=/var/spool/slurmd
SlurmctldPort=6817
SlurmdPort=6818
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
AccountingStorageType=accounting_storage/none
JobCompType=jobcomp/filetxt
JobCompLoc=/var/log/slurm/jobcomp.log

# Node definition for this host; add more NodeName lines for other nodes
NodeName=$HOSTNAME_NOW CPUs=$CORES RealMemory=$REAL_MEM_MB NodeAddr=${TS_NODE_NAME}
PartitionName=main Nodes=ALL Default=YES MaxTime=INFINITE State=UP
EOF

chown slurm:slurm /etc/slurm/slurm.conf
chmod 0644 /etc/slurm/slurm.conf

if [ "$WITH_ACCOUNTING" = "true" ]; then
  echo "==> Accounting requested. You still need MariaDB setup and slurmdbd.conf. See docs/cluster-setup.md."
fi

echo "==> Enabling Slurm services"
if [ "$ROLE" = "controller" ]; then
  systemctl enable --now slurmctld
else
  systemctl enable --now slurmd
fi

echo "==> Done. Verify with: sinfo && scontrol show nodes"
echo "   Tailscale status: tailscale status"
echo "   To copy munge key to other nodes (from controller):"
echo "     ./scripts/scp-munge-key.sh user node1.tailnet.ts.net node2.tailnet.ts.net"

